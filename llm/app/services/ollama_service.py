import os
import ollama
from .llm_service import BaseLLMService
from config import OLLAMA_MODEL, SYSTEM_PROMPT, TEMPERATURE, MAX_TOKENS

OLLAMA_HOST = os.getenv("OLLAMA_HOST")

class OllamaService(BaseLLMService):
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama"""
    
    def __init__(self):
        self.client = ollama.Client(host=OLLAMA_HOST)
        self.model = OLLAMA_MODEL
        self._is_available = False
    
    async def ask(self, prompt: str, context: str = "") -> dict:
        """–ó–∞–ø—Ä–æ—Å –∫ Ollama"""
        try:
            full_prompt = f"{SYSTEM_PROMPT}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {prompt}"
            
            response = self.client.chat(
                model=self.model,
                messages=[{"role": "user", "content": full_prompt}],
                options={
                    'temperature': TEMPERATURE,
                    'num_predict': MAX_TOKENS
                }
            )
            
            ai_text = response['message']['content']
            return self._format_response(ai_text, self.model)
            
        except Exception as e:
            return self._format_error(str(e))
    
    async def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama"""
        return await self.health_check()
    
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Ollama —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        import logging
        logger = logging.getLogger("nutrition-llm")
        
        logger.info(f"ü©∫ –ù–∞—á–∏–Ω–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É Ollama...")
        logger.info(f"üìç OLLAMA_HOST: {OLLAMA_HOST}")
        logger.info(f"üìç OLLAMA_MODEL: {OLLAMA_MODEL}")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ ‚Ññ1: HTTP –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            import requests
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º HTTP –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å...")
            response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=10)
            logger.info(f"üì° HTTP —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ ‚Ññ2: –ß–µ—Ä–µ–∑ ollama client
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ ollama client...")
            models = self.client.list()
            if 'models' in models:
                available_models = [m['name'] for m in models['models']]
            else:
                available_models = []
                logger.warning(f"‚ùå –ö–ª—é—á 'models' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ: {models}")
                
            logger.info(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ ‚Ññ3: –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
            model_available = self.model in available_models
            logger.info(f"üîç –ú–æ–¥–µ–ª—å {self.model} –¥–æ—Å—Ç—É–ø–Ω–∞: {model_available}")
            
            self._is_available = model_available
            return model_available
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            logger.error("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
            logger.error("   - –ó–∞–ø—É—â–µ–Ω –ª–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä Ollama")
            logger.error("   - –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ —Å–µ—Ç—å Docker")
            logger.error("   - –î–æ—Å—Ç—É–ø–µ–Ω –ª–∏ Ollama –ø–æ –∞–¥—Ä–µ—Å—É: " + OLLAMA_HOST)
            return False
            
        except requests.exceptions.Timeout as e:
            logger.error(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return False
            
        except Exception as e:
            logger.error(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False