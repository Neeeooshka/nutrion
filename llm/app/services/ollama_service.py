import os
import asyncio
import aiohttp
import json
import logging
from .llm_service import BaseLLMService
from config import OLLAMA_MODEL, SYSTEM_PROMPT, TEMPERATURE, MAX_TOKENS

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")

class OllamaService(BaseLLMService):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama"""

    def __init__(self):
        self.model = OLLAMA_MODEL
        self._is_available = False
        self.logger = logging.getLogger("nutrition-llm")

    async def ask(self, prompt: str, context: str = "") -> dict:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Ollama —á–µ—Ä–µ–∑ HTTP —Å—Ç—Ä–∏–º"""
        self.logger.info(f"‚öôÔ∏è –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama ({self.model}) —á–µ—Ä–µ–∑ aiohttp")

        full_prompt = f"{SYSTEM_PROMPT}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {prompt}"

        url = f"{OLLAMA_HOST}/api/chat"
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": full_prompt},
            ],
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            ],
            "stream": True
        }

        text_accum = ""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=None)) as resp:
                    async for line in resp.content:
                        line = line.decode("utf-8").strip()
                        if not line:
                            continue
                        try:
                            chunk = json.loads(line)
                        except json.JSONDecodeError:
                            # –±—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—Ç—Ä–∏–º —à–ª—ë—Ç –∫—É—Å–∫–∏ –±–µ–∑ \n –º–µ–∂–¥—É –Ω–∏–º–∏
                            continue

                        if "message" in chunk and "content" in chunk["message"]:
                            piece = chunk["message"]["content"]
                            text_accum += piece
                            self.logger.debug(f"üí¨ {piece.strip()}")

                        if chunk.get("done"):
                            break

            self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–æ–π {len(text_accum)} —Å–∏–º–≤–æ–ª–æ–≤")
            return self._format_response(text_accum, self.model)

        except asyncio.TimeoutError:
            self.logger.warning("‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama")
            return self._format_error("–ò—Å—Ç–µ–∫ —Ç–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç –º–æ–¥–µ–ª–∏")
        except aiohttp.ClientError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ HTTP –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama: {e}")
            return self._format_error(str(e))
        except Exception as e:
            self.logger.exception(e)
            return self._format_error(str(e))

    async def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama"""
        return await self.health_check()

    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Ollama"""
        url = f"{OLLAMA_HOST}/api/generate"
        self.logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ {self.model}...")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json={"model": self.model, "prompt": "ping", "stream": False}) as resp:
                    if resp.status == 200:
                        self._is_available = True
                        self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model} –¥–æ—Å—Ç—É–ø–Ω–∞")
                        return True
        except Exception as e:
            self.logger.warning(f"‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")

        self._is_available = False
        return False
